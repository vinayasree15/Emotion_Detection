{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":1351797,"sourceType":"datasetVersion","datasetId":786787},{"sourceId":4957114,"sourceType":"datasetVersion","datasetId":2820176}],"dockerImageVersionId":30617,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ResNet18","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom torchvision.datasets import ImageFolder\nfrom torch.optim import lr_scheduler\nimport matplotlib.pyplot as plt\nfrom torchvision.models import densenet121 \nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2023-12-10T02:55:22.157601Z","iopub.execute_input":"2023-12-10T02:55:22.158270Z","iopub.status.idle":"2023-12-10T02:55:49.289301Z","shell.execute_reply.started":"2023-12-10T02:55:22.158235Z","shell.execute_reply":"2023-12-10T02:55:49.288276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Grayscale(1),  \n    transforms.Resize((48, 48)),  \n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),  \n    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1), \n    transforms.ToTensor() \n])\n\ntrain_dataset = ImageFolder(root='/kaggle/input/fer2013/train', transform=transform)\ntest_dataset = ImageFolder(root='/kaggle/input/fer2013/test', transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, dropout_rate=0.5):  \n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.downsample = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            self.downsample = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n\n    def forward(self, x):\n        identity = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out += self.downsample(identity)\n        out = self.relu(out)\n        return out\n    \nclass DeeperResNet(nn.Module):\n    def __init__(self, block, layers, num_classes=7, dropout_rate=0.5): \n        super(DeeperResNet, self).__init__()\n        self.in_channels = 64\n        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(128, num_classes)\n\n\n    def _make_layer(self, block, out_channels, blocks, stride):\n        layers = []\n        layers.append(block(self.in_channels, out_channels, stride))\n        self.in_channels = out_channels\n        for _ in range(1, blocks):\n            layers.append(block(out_channels, out_channels))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2023-12-10T02:56:55.343772Z","iopub.execute_input":"2023-12-10T02:56:55.344660Z","iopub.status.idle":"2023-12-10T02:57:06.649005Z","shell.execute_reply.started":"2023-12-10T02:56:55.344628Z","shell.execute_reply":"2023-12-10T02:57:06.648268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = DeeperResNet(BasicBlock, [2, 2], num_classes=7, dropout_rate=0.3).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\nscheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T02:57:16.744784Z","iopub.execute_input":"2023-12-10T02:57:16.745331Z","iopub.status.idle":"2023-12-10T04:09:44.790527Z","shell.execute_reply.started":"2023-12-10T02:57:16.745277Z","shell.execute_reply":"2023-12-10T04:09:44.789574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_losses = []\ntest_losses = []\ntest_accuracies = []\ntrain_accuracies = []\n\nnum_epochs = 50\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    correct_train = 0\n    total_train = 0\n\n    for i, (inputs, labels) in enumerate(train_loader, 0):\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n        _, predicted = torch.max(outputs, 1)\n        total_train += labels.size(0)\n        correct_train += (predicted == labels).sum().item()\n\n        if i % 100 == 99:\n            print('[%d, %5d] loss: %.3f - Train Accuracy: %.2f %%' %\n                  (epoch + 1, i + 1, running_loss / 100, (100 * correct_train / total_train)))\n            running_loss = 0.0\n\n    scheduler.step()\n\n    model.eval()\n    correct_test = 0\n    total_test = 0\n    test_running_loss = 0.0\n\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            total_test += labels.size(0)\n            correct_test += (predicted == labels).sum().item()\n\n            test_loss = criterion(outputs, labels)\n            test_running_loss += test_loss.item()\n\n        avg_test_loss = test_running_loss / len(test_loader)\n        test_losses.append(avg_test_loss)\n\n        test_accuracy = (100 * correct_test) / total_test\n        test_accuracies.append(test_accuracy)\n\n    train_accuracy = (100 * correct_train) / total_train\n    train_accuracies.append(train_accuracy)\n    train_losses.append(running_loss)\n\n\nhighest_test_accuracy = max(test_accuracies)\nleast_test_loss = min(test_losses)\n\nprint(' Accuracy on the test images: %.2f %%' % highest_test_accuracy)\nprint(' Loss on the test images: %.4f' % least_test_loss)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(train_losses, label='Train Loss')\nplt.plot(test_losses, label='Test Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Test Losses')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(train_accuracies, label='Train Accuracy')\nplt.plot(test_accuracies, label='Test Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Training and Test Accuracies')\nplt.legend()\n\nplt.tight_layout() \nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-10T04:17:15.024661Z","iopub.execute_input":"2023-12-10T04:17:15.025054Z","iopub.status.idle":"2023-12-10T04:17:15.436233Z","shell.execute_reply.started":"2023-12-10T04:17:15.025026Z","shell.execute_reply":"2023-12-10T04:17:15.435483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'ResNet18Adam.pth')","metadata":{"execution":{"iopub.status.busy":"2023-12-06T01:59:29.285658Z","iopub.execute_input":"2023-12-06T01:59:29.285948Z","iopub.status.idle":"2023-12-06T01:59:29.307060Z","shell.execute_reply.started":"2023-12-06T01:59:29.285917Z","shell.execute_reply":"2023-12-06T01:59:29.306347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SGD","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = DeeperResNet(BasicBlock, [2, 2], num_classes=7, dropout_rate=0.3).to(device)  \ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-5)\nscheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2023-12-06T01:59:29.307972Z","iopub.execute_input":"2023-12-06T01:59:29.308248Z","iopub.status.idle":"2023-12-06T01:59:32.230106Z","shell.execute_reply.started":"2023-12-06T01:59:29.308201Z","shell.execute_reply":"2023-12-06T01:59:32.229316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lists to store training loss, test loss, test accuracy, and train accuracy\ntrain_losses = []\ntest_losses = []\ntest_accuracies = []\ntrain_accuracies = []\n\nnum_epochs = 50\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    correct_train = 0\n    total_train = 0\n\n    for i, (inputs, labels) in enumerate(train_loader, 0):\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n        _, predicted = torch.max(outputs, 1)\n        total_train += labels.size(0)\n        correct_train += (predicted == labels).sum().item()\n\n        if i % 100 == 99:\n            print('[%d, %5d] loss: %.3f - Train Accuracy: %.2f %%' %\n                  (epoch + 1, i + 1, running_loss / 100, (100 * correct_train / total_train)))\n            running_loss = 0.0\n\n    scheduler.step()\n\n    # Testing accuracy and loss after each epoch\n    model.eval()\n    correct_test = 0\n    total_test = 0\n    test_running_loss = 0.0\n\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            total_test += labels.size(0)\n            correct_test += (predicted == labels).sum().item()\n\n            test_loss = criterion(outputs, labels)\n            test_running_loss += test_loss.item()\n\n        avg_test_loss = test_running_loss / len(test_loader)\n        test_losses.append(avg_test_loss)\n\n        test_accuracy = (100 * correct_test) / total_test\n        test_accuracies.append(test_accuracy)\n\n    train_accuracy = (100 * correct_train) / total_train\n    train_accuracies.append(train_accuracy)\n    train_losses.append(running_loss)\n\n\nhighest_test_accuracy = max(test_accuracies)\nleast_test_loss = min(test_losses)\n\nprint(' Accuracy on the test images: %.2f %%' % highest_test_accuracy)\nprint(' Loss on the test images: %.4f' % least_test_loss)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-06T01:59:32.231188Z","iopub.execute_input":"2023-12-06T01:59:32.231456Z","iopub.status.idle":"2023-12-06T02:25:21.495299Z","shell.execute_reply.started":"2023-12-06T01:59:32.231422Z","shell.execute_reply":"2023-12-06T02:25:21.494313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nplt.figure(figsize=(15, 5))\nplt.subplot(1, 2, 1)\nplt.plot(train_losses, label='Train Loss')\nplt.plot(test_losses, label='Test Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Test Losses')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(train_accuracies, label='Train Accuracy')\nplt.plot(test_accuracies, label='Test Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Training and Test Accuracies')\nplt.legend()\n\nplt.tight_layout()  \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-06T02:26:27.724658Z","iopub.execute_input":"2023-12-06T02:26:27.725071Z","iopub.status.idle":"2023-12-06T02:26:28.102536Z","shell.execute_reply.started":"2023-12-06T02:26:27.725039Z","shell.execute_reply":"2023-12-06T02:26:28.101797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'ResNetSGD.pth')","metadata":{"execution":{"iopub.status.busy":"2023-12-06T02:26:33.199232Z","iopub.execute_input":"2023-12-06T02:26:33.199677Z","iopub.status.idle":"2023-12-06T02:26:33.212697Z","shell.execute_reply.started":"2023-12-06T02:26:33.199635Z","shell.execute_reply":"2023-12-06T02:26:33.211941Z"},"trusted":true},"execution_count":null,"outputs":[]}]}